# -*- coding: utf-8 -*-
"""Tensorflow_Pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7YjuWiQnUo8FC32oYpv2KmH_fN-7zHj

#Introduction to TensorFlow

# üìò TensorFlow Basics

TensorFlow is a powerful open-source library developed by Google for deep learning and numerical computation. It uses dataflow graphs and automatic differentiation to perform optimized computations.

Let's get started by learning how to use Tensors, which are the fundamental building blocks of TensorFlow.

##Installing and Importing TensorFlow
"""

!pip install tensorflow -q
import tensorflow as tf
print("TensorFlow version:", tf.__version__)

"""## üì¶ Tensors

Tensors are multi-dimensional arrays (like NumPy arrays). TensorFlow supports scalar (0D), vector (1D), matrix (2D), and higher dimensional tensors.

Let's see how to create and inspect them.

##Tensor Creation and Attributes
"""

# Scalar
scalar = tf.constant(7)
print("Scalar:", scalar)

# Vector
vector = tf.constant([1.0, 2.0, 3.0])
print("Vector:", vector)

# Matrix
matrix = tf.constant([[1, 2], [3, 4]])
print("Matrix:\n", matrix)

# Check shape and dtype
print("Shape:", matrix.shape)
print("Data Type:", matrix.dtype)

"""## ‚ûï Tensor Operations

TensorFlow supports element-wise operations like addition, multiplication, matrix multiplication, etc.

##Tensor Arithmetic
"""

a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Addition
print("Addition:\n", tf.add(a, b))

# Multiplication (element-wise)
print("Element-wise Multiplication:\n", tf.multiply(a, b))

# Matrix multiplication
print("Matrix Multiplication:\n", tf.matmul(a, b))

"""# üèóÔ∏è Building a Neural Network with TensorFlow (Keras)

We'll use `tf.keras`, a high-level API for building and training models. Let's train a simple neural network to classify digits using the MNIST dataset.

##Load Dataset and Normalize
"""

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize to [0, 1]
x_train, x_test = x_train / 255.0, x_test / 255.0

print(x_train.shape)

"""##Define a Model

"""

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),  # Input layer
    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer
    tf.keras.layers.Dense(10, activation='softmax') # Output layer
])

"""##Compile and Train"""

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5)

"""##Evaluate the Model"""

test_loss, test_acc = model.evaluate(x_test, y_test)
print("\nTest Accuracy:", test_acc)

"""#1. Understand the Basic Syntax and Operations (PyTorch)

# Introduction to PyTorch
## üî• PyTorch Basics

PyTorch is a popular open-source deep learning library developed by Facebook AI. It provides flexibility and speed through dynamic computation graphs and strong GPU support.

In this section, we'll start by exploring how PyTorch handles tensors and automatic differentiation.

##Installing and Importing PyTorch
"""

# If you're in a notebook or Colab
!pip install torch -q
import torch
print("PyTorch version:", torch.__version__)

"""##Creating and Inspecting Tensors
## üì¶ Tensors in PyTorch

Tensors are multidimensional arrays, just like in NumPy. They're the building blocks of PyTorch and support operations like addition, multiplication, reshaping, etc.

##Tensor Creation and Properties
"""

# Scalar
scalar = torch.tensor(5)
print("Scalar:", scalar)

# Vector
vector = torch.tensor([1.0, 2.0, 3.0])
print("Vector:", vector)

# Matrix
matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
print("Matrix:\n", matrix)

# Shape and data type
print("Shape:", matrix.shape)
print("Data Type:", matrix.dtype)

"""## ‚ûï Tensor Operations

PyTorch supports common element-wise operations as well as matrix multiplication, reshaping, and broadcasting.

##Tensor Arithmetic
"""

a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)

# Addition
print("Addition:\n", a + b)

# Element-wise multiplication
print("Element-wise Multiplication:\n", a * b)

# Matrix multiplication
print("Matrix Multiplication:\n", torch.matmul(a, b))

"""# üèóÔ∏è Building a Neural Network with PyTorch

Now let's build a simple neural network to classify digits from the MNIST dataset using `torch.nn` and `torch.optim`.

##Load and Normalize MNIST Dataset
"""

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Transform: Normalize to [0, 1]
transform = transforms.ToTensor()

train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

"""##Define Neural Network"""

import torch.nn as nn
import torch.nn.functional as F

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(x.shape[0], -1)  # Flatten
        x = F.relu(self.fc1(x))
        return F.log_softmax(self.fc2(x), dim=1)

model = SimpleNN()

"""##Define Loss and Optimizer"""

import torch.optim as optim

criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

"""##Train the Model"""

epochs = 3
for epoch in range(epochs):
    for images, labels in train_loader:
        optimizer.zero_grad()
        output = model(images)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

"""##Evaluate Accuracy"""

correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {correct / total * 100:.2f}%")

"""#3. Compare Similarities and Differences with TensorFlow


# PyTorch vs TensorFlow: Quick Comparison

| Feature                | PyTorch                               | TensorFlow                           |
|------------------------|----------------------------------------|--------------------------------------|
| Execution Mode         | Eager (Dynamic)                        | Eager (Static with option for Graph) |
| High-Level API         | `torch.nn`, `torch.optim`              | `tf.keras`                           |
| Dataset Loader         | `DataLoader`                           | `tf.data.Dataset`                    |
| Training Loop Style    | Mostly manual                          | `model.fit()` or manual              |
| Model Saving           | `torch.save()`                         | `model.save()` or `SavedModel`       |
| Visualization Tool     | TensorBoard (via `torch.utils.tensorboard`) | TensorBoard                    |
| Autograd Support       | Built-in via `autograd`                | `tf.GradientTape()`                  |

Both libraries are capable of building state-of-the-art models. Your choice depends on project needs and comfort with syntax.

--------------------------------------------------------------------------------

#Activity

Create two custom models, one with Tensorflow and the other with Pytorch.
The models should have

###  Model Architecture (Same for Both Frameworks)

| Layer Type         | Configuration                      |
|--------------------|-------------------------------------|
| Input Layer        | Flatten (28√ó28 ‚Üí 784)              |
| Dense Layer 1      | 256 units, ReLU                    |
| Dropout Layer      | 30% (rate = 0.3)                   |
| Dense Layer 2      | 128 units, Tanh                    |
| Output Layer       | 10 units, Softmax (TF) / LogSoftmax (PT) |

###  Instructions

- Implement this architecture using `tf.keras.Sequential` in TensorFlow.
- Implement the same architecture using `torch.nn.Module` in PyTorch.
- Train both models for **5 epochs** using the **Adam optimizer** and an appropriate classification loss.
- Evaluate accuracy on the MNIST test set.
"""

# TensorFlow version
import tensorflow as tf
from tensorflow.keras import layers, models

# Load MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize

# Build model
model_tf = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='tanh'),
    layers.Dense(10, activation='softmax')  # Softmax for classification
])

# Compile
model_tf.compile(optimizer='adam',
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

# Train
model_tf.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)

# Evaluate
test_loss_tf, test_acc_tf = model_tf.evaluate(x_test, y_test, verbose=2)
print(f"TensorFlow Test Accuracy: {test_acc_tf:.4f}")

# PyTorch version
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Transforms and Loaders
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

# Model Definition
class NetPT(nn.Module):
    def __init__(self):
        super(NetPT, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 256)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, 128)
        self.output = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.tanh(self.fc2(x))
        x = F.log_softmax(self.output(x), dim=1)  # LogSoftmax
        return x

model_pt = NetPT().to(device)
optimizer = torch.optim.Adam(model_pt.parameters())
criterion = nn.NLLLoss()

# Training Loop
for epoch in range(5):
    model_pt.train()
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model_pt(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1} complete")


model_pt.eval()
correct = 0
total = 0
with torch.no_grad():
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        output = model_pt(data)
        pred = output.argmax(dim=1)
        correct += (pred == target).sum().item()
        total += target.size(0)

accuracy_pt = correct / total
print(f"PyTorch Test Accuracy: {accuracy_pt:.4f}")
#made both optimal through chatGPT

#Pytorch has slightly high test accuracy